{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickbait Spoiler Generation using LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary to fix the imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_CHECKPOINT = \"decapoda-research/llama-7b-hf\"\n",
    "SAVE_CHECKPOINT_PATH = \"../src/models/llama/clickbait\"\n",
    "\n",
    "# https://github.com/tloen/alpaca-lora/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n",
      "Training Alpaca-LoRA model with params:\n",
      "base_model: decapoda-research/llama-7b-hf\n",
      "data_path: ../data/parsed/llama/clickbait_data.py\n",
      "output_dir: ../src/models/llama/clickbait\n",
      "batch_size: 3\n",
      "micro_batch_size: 4\n",
      "num_epochs: 5\n",
      "learning_rate: 2e-05\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n",
      "Downloading shards:   0%|                                | 0/33 [00:00<?, ?it/s]\n",
      "Downloading (…)l-00001-of-00033.bin:   0%|           | 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:   3%|  | 10.5M/405M [00:01<00:47, 8.32MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:   5%|  | 21.0M/405M [00:01<00:33, 11.5MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:   8%|▏ | 31.5M/405M [00:02<00:30, 12.4MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  10%|▏ | 41.9M/405M [00:04<00:37, 9.56MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  13%|▎ | 52.4M/405M [00:05<00:36, 9.68MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  16%|▎ | 62.9M/405M [00:06<00:35, 9.64MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  18%|▎ | 73.4M/405M [00:07<00:32, 10.3MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  21%|▍ | 83.9M/405M [00:08<00:29, 10.9MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  23%|▍ | 94.4M/405M [00:08<00:26, 11.7MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  26%|▊  | 105M/405M [00:09<00:23, 12.8MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  28%|▊  | 115M/405M [00:10<00:24, 11.8MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  31%|▉  | 126M/405M [00:11<00:26, 10.4MB/s]\u001b[A\n",
      "Downloading (…)l-00001-of-00033.bin:  34%|█  | 136M/405M [00:12<00:26, 10.0MB/s]\u001b[A^C\n",
      "Downloading shards:   0%|                                | 0/33 [00:13<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/notebooks/../src/models/llama/alpaca-lora/finetune.py\", line 283, in <module>\n",
      "    fire.Fire(train)\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/notebooks/../src/models/llama/alpaca-lora/finetune.py\", line 112, in train\n",
      "    model = LlamaForCausalLM.from_pretrained(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2523, in from_pretrained\n",
      "    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 934, in get_checkpoint_shard_files\n",
      "    cached_filename = cached_file(\n",
      "                      ^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1364, in hf_hub_download\n",
      "    http_get(\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 541, in http_get\n",
      "    for chunk in r.iter_content(chunk_size=10 * 1024 * 1024):\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/requests/models.py\", line 816, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/urllib3/response.py\", line 935, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/urllib3/response.py\", line 874, in read\n",
      "    data = self._raw_read(amt)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/urllib3/response.py\", line 809, in _raw_read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/urllib3/response.py\", line 794, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 466, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1278, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1134, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Downloading (…)l-00001-of-00033.bin:  34%|█  | 136M/405M [00:13<00:26, 10.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../src/models/llama/alpaca-lora/finetune.py \\\n",
    "    --base_model $MODEL_CHECKPOINT \\\n",
    "    --data_path '../data/parsed/llama/clickbait_data.py' \\\n",
    "    --output_dir $SAVE_CHECKPOINT_PATH \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --num_epochs $N_EPOCHS \\\n",
    "    --learning_rate $LEARNING_RATE \\\n",
    "    --cutoff_len 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py \\\n",
    "    --load_8bit \\\n",
    "    --base_model $MODEL_CHECKPOINT  \\\n",
    "    --lora_weights $SAVE_CHECKPOINT_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
