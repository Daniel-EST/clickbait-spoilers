{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickbait Spoiler Generation using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary to fix the imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"distilbert-base-uncased-distilled-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "SAVE_CHECKPOINT_PATH = \"../src/models/bert/clickbait\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bert import MODEL_CHECKPOINT, TOKENIZER\n",
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"../data/parsed/bert/clickbait_data.py\", data_files={\n",
    "    \"train\": \"train.jsonl\", \n",
    "    \"test\": \"test.jsonl\", \n",
    "    \"validation\": \"validation.jsonl\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.bert\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    utils.bert.preprocess_training,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = raw_datasets[\"test\"].map(\n",
    "    utils.bert.preprocess_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"test\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"test\"]), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    utils.bert.preprocess_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_steps = len(tf_train_dataset) * N_EPOCHS\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=LEARNING_RATE,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=\"../src/models/bert/checkpoints/model.{epoch:02d}_{accuracy:.2f}-{loss:.2f}_{val_accuracy:.2f}-{val_loss:.2f}.h5\"),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"../src/models/bert/logs\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    callbacks=callbacks,\n",
    "    epochs=N_EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(SAVE_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_dataset = model.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.bert.compute_metrics(\n",
    "    predictions[\"start_logits\"],\n",
    "    predictions[\"end_logits\"],\n",
    "    validation_dataset,\n",
    "    raw_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=SAVE_CHECKPOINT_PATH, tokenizer=TOKENIZER)\n",
    "\n",
    "context = \"\"\"\n",
    "At some point toward the end of every job interview, the hiring manager will likely turn the tables and ask, \\\"Do you have any questions for me?\\\"\\nThis is the time to ask smart, thoughtful questions — perhaps your final opportunity to assess whether the job would be a good fit, and your final chance to impress the hiring manager.\\nBusiness Insider recently spoke with Becca Brown, cofounder of Solemates, a brand of women's shoe-care products, who knows a thing or two about interviewing.\\nBefore launching her own business, Brown, who has a bachelor's from Harvard University and an MBA from Columbia, spent a lot of time interviewing job candidates at Goldman Sachs, where she held various roles, including analyst, wealth adviser, and chief of staff.\\nShe was also part of the investment bank's Harvard recruiting team, she says.\\n\\\"I interviewed anywhere from 20 to 30 job candidates a year, so in total, I interviewed over 100 people at Goldman Sachs,\\\" she tells Business Insider.\\nShe says that candidates asked her some impressive questions — like \\\"What's the most challenging part of your job?\\\" and \\\"What's one of the most interesting projects you've worked on?\\\" — but there was one question she always hoped she'd be asked, but almost never was: \\\"Where do you see yourself in five years?\\\"\\n\\\"I like this question — and yet no one ever asked it — because it's difficult to answer,\\\" she says. \\\"It's an important question for anyone to be asking him or herself, and so if ever a candidate were to ask this question, it would have stood out.\\\"\\nShe continues:\\nI think this is a good question for interviewees to ask because, as a candidate, if you see where the person interviewing you is headed, you can decide if that trajectory is in line with your career objectives. While they don't have to be completely correlated, it's helpful for the candidate to have some indication of the interviewer's direction.\\nGet the latest Goldman Sachs stock price here.\n",
    "\"\"\"\n",
    "question=\"A woman who interviewed over 100 people at Goldman Sachs says there's one question she always hoped candidates would ask her, but they never did\"\n",
    "expected = \"\\\"Where do you see yourself in five years?\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = question_answerer(question=question, context=context)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "# bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_results = meteor.compute(predictions=[prediction[\"answer\"]], references=[expected])\n",
    "bleu_results = bleu.compute(predictions=[prediction[\"answer\"]], references=[expected])\n",
    "# bertscore_results = bertscore.compute(predictions=prediction, references=expected, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{question};\\n\\nExpected Spoiler: {expected};\\n\\nSpoiler Predicted: {prediction['answer']};\\n\\n###\\n\\n\")\n",
    "print(f\"Meteor: {meteor_results['meteor']}\\nBLEU-4: {bleu_results['bleu']}\")\n",
    "# print(f\"Meteor: {meteor_results['meteor']}\\nBLEU-4: {bleu_results['bleu']}\\nBERTscore Mean F1: {sum(bertscore_results['f1'])/len(bertscore_results['f1'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
