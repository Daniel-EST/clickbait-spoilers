{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickbait Spoiler Generation using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary to fix the imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "SAVE_CHECKPOINT_PATH = \"../src/models/bert/clickbait\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ddsantos/Coding/projects/clickbait-spoiler/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ddsantos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ddsantos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ddsantos/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDebertaV2ForQuestionAnswering: ['deberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFDebertaV2ForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDebertaV2ForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDebertaV2ForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2ForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from utils.bert import MODEL_CHECKPOINT, TOKENIZER\n",
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset clickbait_data (/Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7)\n",
      "100%|██████████| 3/3 [00:00<00:00, 214.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"../data/parsed/bert/clickbait_data.py\", data_files={\n",
    "    \"train\": \"train.jsonl\", \n",
    "    \"test\": \"test.jsonl\", \n",
    "    \"validation\": \"validation.jsonl\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7/cache-62bd1b26668cb0d5.arrow\n",
      "Loading cached processed dataset at /Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7/cache-17d049326d16e504.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1367, 3070)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils.bert\n",
    "\n",
    "raw_train = raw_datasets[\"train\"].filter(lambda x: x[\"type\"]==\"phrase\")\n",
    "train_dataset = raw_train.map(\n",
    "    utils.bert.preprocess_training,\n",
    "    batched=True,\n",
    "    remove_columns=raw_train.column_names,\n",
    ")\n",
    "len(raw_train), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7/cache-6de2b26074509df4.arrow\n",
      "Loading cached processed dataset at /Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7/cache-d685757585e95062.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(268, 561)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test = raw_datasets[\"test\"].filter(lambda x: x[\"type\"]==\"phrase\")\n",
    "test_dataset = raw_test.map(\n",
    "    utils.bert.preprocess_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_test.column_names,\n",
    ")\n",
    "len(raw_test), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7/cache-42b1cad1e19a8d9f.arrow\n",
      "Loading cached processed dataset at /Users/ddsantos/.cache/huggingface/datasets/clickbait_data/default-e72966e7874160b4/0.0.0/2132235c1c29143999b3a1b191327cbddac13587917dfde07b49cb535c8668f7/cache-2b958758727bd9bb.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67, 172)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_validation = raw_datasets[\"validation\"].filter(lambda x: x[\"type\"]==\"phrase\")\n",
    "validation_dataset = raw_validation.map(\n",
    "    utils.bert.preprocess_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_validation.column_names,\n",
    ")\n",
    "len(raw_validation), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_steps = len(tf_train_dataset) * N_EPOCHS\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=LEARNING_RATE,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"../src/models/bert/logs\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     tf_train_dataset,\n",
    "#     callbacks=callbacks,\n",
    "#     epochs=N_EPOCHS,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(SAVE_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDebertaV2ForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDebertaV2ForQuestionAnswering were initialized from the model checkpoint at ../src/models/bert/clickbait.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2ForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(SAVE_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_dataset = model.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:53:39.460264: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 491s 3s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:02<00:00, 119.80it/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 37.5kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 474/474 [00:00<00:00, 1.75MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:01<00:00, 870kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.09MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 559M/559M [00:50<00:00, 11.1MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SQUAD': {'exact_match': 9.328358208955224, 'f1': 12.50087479180982},\n",
       " 'Meteor': {'meteor': 0.10885022442133244},\n",
       " 'BLEU-4': {'bleu': 0.03323292412639024,\n",
       "  'precisions': [0.1091127098321343,\n",
       "   0.061837455830388695,\n",
       "   0.020202020202020204,\n",
       "   0.008948545861297539],\n",
       "  'brevity_penalty': 1.0,\n",
       "  'length_ratio': 1.215743440233236,\n",
       "  'translation_length': 834,\n",
       "  'reference_length': 686},\n",
       " 'BERTscore': {'precision': [0.9999998211860657,\n",
       "   0.6120501756668091,\n",
       "   0.9978950023651123,\n",
       "   0.7967748045921326,\n",
       "   0.7692604660987854,\n",
       "   0.9978941679000854,\n",
       "   0.7765107154846191,\n",
       "   0.8461945652961731,\n",
       "   0.8176360726356506,\n",
       "   0.5526825189590454,\n",
       "   0.9999997615814209,\n",
       "   0.8616260886192322,\n",
       "   1.0000001192092896,\n",
       "   0.8160631060600281,\n",
       "   0.8596373796463013,\n",
       "   0.9978932738304138,\n",
       "   0.8572559356689453,\n",
       "   0.8181390166282654,\n",
       "   0.9978954792022705,\n",
       "   0.745299220085144,\n",
       "   0.578849732875824,\n",
       "   0.9416104555130005,\n",
       "   0.803884744644165,\n",
       "   0.8083292841911316,\n",
       "   0.8940995931625366,\n",
       "   0.5969404578208923,\n",
       "   0.7550269365310669,\n",
       "   0.538474440574646,\n",
       "   0.8766553401947021,\n",
       "   0.7348096370697021,\n",
       "   0.8494231104850769,\n",
       "   0.8317741751670837,\n",
       "   0.889873206615448,\n",
       "   0.7361068725585938,\n",
       "   0.7842487692832947,\n",
       "   0.9978960752487183,\n",
       "   0.690685510635376,\n",
       "   0.7809606790542603,\n",
       "   0.5464668869972229,\n",
       "   1.000000238418579,\n",
       "   0.792163610458374,\n",
       "   0.8466397523880005,\n",
       "   0.8854277729988098,\n",
       "   0.7958563566207886,\n",
       "   0.5682995319366455,\n",
       "   0.6900060176849365,\n",
       "   0.8536909818649292,\n",
       "   0.6738772988319397,\n",
       "   0.9978952407836914,\n",
       "   0.461008220911026,\n",
       "   0.9978907704353333,\n",
       "   0.5535449981689453,\n",
       "   0.8293041586875916,\n",
       "   0.9978920817375183,\n",
       "   0.70868980884552,\n",
       "   0.7265596389770508,\n",
       "   0.9978950023651123,\n",
       "   0.7979551553726196,\n",
       "   0.7599858045578003,\n",
       "   0.9999998211860657,\n",
       "   0.5704904198646545,\n",
       "   0.8061915040016174,\n",
       "   0.8163067698478699,\n",
       "   0.8413994312286377,\n",
       "   0.7937486171722412,\n",
       "   0.5503234267234802,\n",
       "   0.9978955984115601,\n",
       "   0.6327617168426514,\n",
       "   0.7449498772621155,\n",
       "   0.7741712927818298,\n",
       "   0.6325446367263794,\n",
       "   0.5790565013885498,\n",
       "   0.9999998807907104,\n",
       "   0.9978976249694824,\n",
       "   0.5609570741653442,\n",
       "   0.8487552404403687,\n",
       "   0.7428829669952393,\n",
       "   0.6149505376815796,\n",
       "   0.9999998807907104,\n",
       "   0.7864257097244263,\n",
       "   0.8130468130111694,\n",
       "   0.671495795249939,\n",
       "   0.5887009501457214,\n",
       "   0.6254482269287109,\n",
       "   0.7945317029953003,\n",
       "   1.000000238418579,\n",
       "   0.8517137169837952,\n",
       "   0.9978958964347839,\n",
       "   0.8048405051231384,\n",
       "   0.9999999403953552,\n",
       "   0.6273847222328186,\n",
       "   0.7535204291343689,\n",
       "   0.7035489678382874,\n",
       "   0.8751919865608215,\n",
       "   0.7827265858650208,\n",
       "   0.88294917345047,\n",
       "   0.7370464205741882,\n",
       "   0.7834748029708862,\n",
       "   0.6103577613830566,\n",
       "   0.817333459854126,\n",
       "   0.5939340591430664,\n",
       "   0.5583582520484924,\n",
       "   0.8357195854187012,\n",
       "   0.9999996423721313,\n",
       "   0.5669232606887817,\n",
       "   0.8109006285667419,\n",
       "   0.7030089497566223,\n",
       "   0.5882375240325928,\n",
       "   0.7418398261070251,\n",
       "   0.997894823551178,\n",
       "   0.5666965842247009,\n",
       "   0.9978938698768616,\n",
       "   0.8121594786643982,\n",
       "   0.8428043127059937,\n",
       "   0.8135920763015747,\n",
       "   0.6766682267189026,\n",
       "   0.8296058177947998,\n",
       "   0.7845821380615234,\n",
       "   1.000000238418579,\n",
       "   0.5655635595321655,\n",
       "   0.6090654134750366,\n",
       "   0.997896134853363,\n",
       "   1.0,\n",
       "   0.8413683772087097,\n",
       "   0.997895359992981,\n",
       "   0.9999998211860657,\n",
       "   0.9978883266448975,\n",
       "   0.7189507484436035,\n",
       "   0.7837275862693787,\n",
       "   0.5949569940567017,\n",
       "   0.9978957772254944,\n",
       "   0.9978943467140198,\n",
       "   0.9978960752487183,\n",
       "   0.6124252080917358,\n",
       "   0.5700348615646362,\n",
       "   0.8509445190429688,\n",
       "   0.8227357864379883,\n",
       "   0.8597121834754944,\n",
       "   0.6318070292472839,\n",
       "   0.5837144255638123,\n",
       "   0.8044455051422119,\n",
       "   0.997894287109375,\n",
       "   0.7926144599914551,\n",
       "   0.9978953003883362,\n",
       "   0.8149636387825012,\n",
       "   0.5992281436920166,\n",
       "   0.7832649946212769,\n",
       "   0.6031541228294373,\n",
       "   0.8576956391334534,\n",
       "   0.8099409341812134,\n",
       "   0.9978945255279541,\n",
       "   0.9998044967651367,\n",
       "   0.9999999403953552,\n",
       "   0.8205362558364868,\n",
       "   0.9978963136672974,\n",
       "   0.8270894289016724,\n",
       "   0.8555624485015869,\n",
       "   0.9031274914741516,\n",
       "   0.5509471893310547,\n",
       "   0.7751429677009583,\n",
       "   0.7631577253341675,\n",
       "   0.6820743083953857,\n",
       "   0.8097877502441406,\n",
       "   0.9999996423721313,\n",
       "   0.9978950619697571,\n",
       "   0.6223962306976318,\n",
       "   0.8092557191848755,\n",
       "   0.7803629040718079,\n",
       "   0.6196209788322449,\n",
       "   0.5602959990501404,\n",
       "   1.000000238418579,\n",
       "   0.6886730194091797,\n",
       "   0.6606324315071106,\n",
       "   0.7933000326156616,\n",
       "   0.6812138557434082,\n",
       "   0.6822936534881592,\n",
       "   0.6415987610816956,\n",
       "   0.533234715461731,\n",
       "   0.5707089900970459,\n",
       "   1.0,\n",
       "   0.6349664330482483,\n",
       "   0.9978956580162048,\n",
       "   0.583175539970398,\n",
       "   0.6497902870178223,\n",
       "   0.997894823551178,\n",
       "   0.6245277523994446,\n",
       "   0.9371755123138428,\n",
       "   0.6979080438613892,\n",
       "   0.5989562273025513,\n",
       "   0.4860002398490906,\n",
       "   0.77207350730896,\n",
       "   1.000000238418579,\n",
       "   0.6085087656974792,\n",
       "   0.9978947043418884,\n",
       "   0.8427914977073669,\n",
       "   0.8622006773948669,\n",
       "   0.8404682278633118,\n",
       "   0.9978948831558228,\n",
       "   0.997895359992981,\n",
       "   0.7497031092643738,\n",
       "   0.8587684035301208,\n",
       "   0.7225592732429504,\n",
       "   0.8118899464607239,\n",
       "   0.73309725522995,\n",
       "   0.8418418169021606,\n",
       "   0.8577591180801392,\n",
       "   0.8463400602340698,\n",
       "   0.6548603773117065,\n",
       "   0.5930772423744202,\n",
       "   0.7836248278617859,\n",
       "   0.6052757501602173,\n",
       "   0.8287672996520996,\n",
       "   0.553101658821106,\n",
       "   0.6697607040405273,\n",
       "   0.8618351817131042,\n",
       "   0.8512275815010071,\n",
       "   0.9978951811790466,\n",
       "   0.7419961094856262,\n",
       "   1.0000004768371582,\n",
       "   0.9978988766670227,\n",
       "   0.9999998211860657,\n",
       "   0.789677619934082,\n",
       "   0.8176683187484741,\n",
       "   0.7887591123580933,\n",
       "   1.0,\n",
       "   0.9978960752487183,\n",
       "   0.8040634393692017,\n",
       "   0.9978936314582825,\n",
       "   0.4018620550632477,\n",
       "   0.6979824900627136,\n",
       "   0.8466055393218994,\n",
       "   0.8264570832252502,\n",
       "   0.9999998807907104,\n",
       "   0.9978946447372437,\n",
       "   1.000000238418579,\n",
       "   1.0,\n",
       "   0.8109622001647949,\n",
       "   0.830863356590271,\n",
       "   0.8056871891021729,\n",
       "   0.9978936910629272,\n",
       "   0.8047065734863281,\n",
       "   0.7630209922790527,\n",
       "   0.5770610570907593,\n",
       "   0.6270998120307922,\n",
       "   1.0000001192092896,\n",
       "   0.8388442397117615,\n",
       "   0.579154372215271,\n",
       "   0.7717856168746948,\n",
       "   0.8092698454856873,\n",
       "   0.6108307242393494,\n",
       "   0.48654434084892273,\n",
       "   0.9978940486907959,\n",
       "   0.8038817644119263,\n",
       "   0.8510297536849976,\n",
       "   0.9999995231628418,\n",
       "   0.9978950619697571,\n",
       "   0.8290466070175171,\n",
       "   0.9999992251396179,\n",
       "   0.7819517850875854,\n",
       "   0.8480147123336792,\n",
       "   0.5734215378761292,\n",
       "   0.5983871221542358,\n",
       "   0.7763314843177795,\n",
       "   0.8288593292236328,\n",
       "   0.720109224319458,\n",
       "   0.4757236838340759,\n",
       "   0.9978951811790466,\n",
       "   0.9978955984115601],\n",
       "  'recall': [0.9999998211860657,\n",
       "   0.5416508913040161,\n",
       "   0.5327544808387756,\n",
       "   0.6395692825317383,\n",
       "   0.6337215900421143,\n",
       "   0.5105646252632141,\n",
       "   0.5663062334060669,\n",
       "   0.5857224464416504,\n",
       "   0.6477475762367249,\n",
       "   0.49405303597450256,\n",
       "   0.9999997615814209,\n",
       "   0.5220446586608887,\n",
       "   1.0000001192092896,\n",
       "   0.7113727927207947,\n",
       "   0.5930535197257996,\n",
       "   0.5305590629577637,\n",
       "   0.4665850102901459,\n",
       "   0.5019594430923462,\n",
       "   0.5764176845550537,\n",
       "   0.47844958305358887,\n",
       "   0.757321298122406,\n",
       "   0.9416104555130005,\n",
       "   0.6469718217849731,\n",
       "   0.7250847816467285,\n",
       "   0.5826899409294128,\n",
       "   0.48808908462524414,\n",
       "   0.6200148463249207,\n",
       "   0.7122381925582886,\n",
       "   0.6866550445556641,\n",
       "   0.641432523727417,\n",
       "   0.5944942831993103,\n",
       "   0.6855562925338745,\n",
       "   0.8810494542121887,\n",
       "   0.48846787214279175,\n",
       "   0.5315675139427185,\n",
       "   0.5953123569488525,\n",
       "   0.6785072088241577,\n",
       "   0.39192360639572144,\n",
       "   0.6106692552566528,\n",
       "   1.000000238418579,\n",
       "   0.7057848572731018,\n",
       "   0.805370569229126,\n",
       "   0.568583607673645,\n",
       "   0.6005167961120605,\n",
       "   0.4215424656867981,\n",
       "   0.6338516473770142,\n",
       "   0.6165034174919128,\n",
       "   0.652387261390686,\n",
       "   0.8261501789093018,\n",
       "   0.712138831615448,\n",
       "   0.43050432205200195,\n",
       "   0.7105453014373779,\n",
       "   0.42496457695961,\n",
       "   0.44626930356025696,\n",
       "   0.7289373874664307,\n",
       "   0.601593554019928,\n",
       "   0.5297362804412842,\n",
       "   0.7303214073181152,\n",
       "   0.48735105991363525,\n",
       "   0.9999998211860657,\n",
       "   0.6333962082862854,\n",
       "   0.6530650854110718,\n",
       "   0.5304709076881409,\n",
       "   0.626457691192627,\n",
       "   0.5067264437675476,\n",
       "   0.5733633041381836,\n",
       "   0.569472074508667,\n",
       "   0.6372072100639343,\n",
       "   0.46642422676086426,\n",
       "   0.5458343029022217,\n",
       "   0.6176261305809021,\n",
       "   0.6016783714294434,\n",
       "   0.9999998807907104,\n",
       "   0.5634831190109253,\n",
       "   0.4830939769744873,\n",
       "   0.6682901382446289,\n",
       "   0.6135620474815369,\n",
       "   0.6169254779815674,\n",
       "   0.9999998807907104,\n",
       "   0.49874287843704224,\n",
       "   0.5459425449371338,\n",
       "   0.7137956619262695,\n",
       "   0.5517420172691345,\n",
       "   0.5923523306846619,\n",
       "   0.5154189467430115,\n",
       "   1.000000238418579,\n",
       "   0.5011606812477112,\n",
       "   0.47543641924858093,\n",
       "   0.8043990135192871,\n",
       "   0.9999999403953552,\n",
       "   0.6353820562362671,\n",
       "   0.5633609294891357,\n",
       "   0.6854481101036072,\n",
       "   0.6677113771438599,\n",
       "   0.5518914461135864,\n",
       "   0.6051926016807556,\n",
       "   0.5325335264205933,\n",
       "   0.5326001644134521,\n",
       "   0.6804769039154053,\n",
       "   0.5648834705352783,\n",
       "   0.6886247396469116,\n",
       "   0.433488667011261,\n",
       "   0.7507771253585815,\n",
       "   0.9999996423721313,\n",
       "   0.55888831615448,\n",
       "   0.7388954162597656,\n",
       "   0.6399462223052979,\n",
       "   0.786252498626709,\n",
       "   0.9035316705703735,\n",
       "   0.5777174234390259,\n",
       "   0.5447640419006348,\n",
       "   0.47657662630081177,\n",
       "   0.5331186056137085,\n",
       "   0.5370023250579834,\n",
       "   0.5822850465774536,\n",
       "   0.6706864833831787,\n",
       "   0.52225661277771,\n",
       "   0.7845821380615234,\n",
       "   1.000000238418579,\n",
       "   0.5273385643959045,\n",
       "   0.5081976056098938,\n",
       "   0.5152057409286499,\n",
       "   1.0,\n",
       "   0.8737428784370422,\n",
       "   0.586881160736084,\n",
       "   0.9999998211860657,\n",
       "   0.3592671751976013,\n",
       "   0.598457932472229,\n",
       "   0.5730950832366943,\n",
       "   0.6676822900772095,\n",
       "   0.5403907299041748,\n",
       "   0.5444135665893555,\n",
       "   0.484235942363739,\n",
       "   0.5799750089645386,\n",
       "   0.40485042333602905,\n",
       "   0.6335045099258423,\n",
       "   0.5596013069152832,\n",
       "   0.6536698341369629,\n",
       "   0.7408881783485413,\n",
       "   0.7604084014892578,\n",
       "   0.7230105400085449,\n",
       "   0.6016438603401184,\n",
       "   0.5703365206718445,\n",
       "   0.5548498630523682,\n",
       "   0.5679411292076111,\n",
       "   0.5908224582672119,\n",
       "   0.7981588840484619,\n",
       "   0.6990078091621399,\n",
       "   0.45980018377304077,\n",
       "   0.5949525237083435,\n",
       "   0.5303800106048584,\n",
       "   0.6035556793212891,\n",
       "   0.9999999403953552,\n",
       "   0.6436024904251099,\n",
       "   0.49546563625335693,\n",
       "   0.5948041081428528,\n",
       "   0.5059967041015625,\n",
       "   0.8723636865615845,\n",
       "   0.4502928853034973,\n",
       "   0.5813555121421814,\n",
       "   0.5781283974647522,\n",
       "   0.6674752235412598,\n",
       "   0.5899679660797119,\n",
       "   0.9999996423721313,\n",
       "   0.5654022693634033,\n",
       "   0.6083739995956421,\n",
       "   0.49547380208969116,\n",
       "   0.5479171872138977,\n",
       "   0.5555315613746643,\n",
       "   0.6417099237442017,\n",
       "   1.000000238418579,\n",
       "   0.657461404800415,\n",
       "   0.634382963180542,\n",
       "   0.682347297668457,\n",
       "   0.6812138557434082,\n",
       "   0.6822936534881592,\n",
       "   0.6360101103782654,\n",
       "   0.4921616315841675,\n",
       "   0.5941252708435059,\n",
       "   1.0,\n",
       "   0.6145601868629456,\n",
       "   0.6217249631881714,\n",
       "   0.44028496742248535,\n",
       "   0.6048610210418701,\n",
       "   0.8109015226364136,\n",
       "   0.7612504363059998,\n",
       "   0.9371755123138428,\n",
       "   0.6034438014030457,\n",
       "   0.5861107110977173,\n",
       "   0.6985989212989807,\n",
       "   0.647783637046814,\n",
       "   1.000000238418579,\n",
       "   0.676689863204956,\n",
       "   0.5213503241539001,\n",
       "   0.6013565063476562,\n",
       "   0.568408727645874,\n",
       "   0.5948821306228638,\n",
       "   0.5675365924835205,\n",
       "   0.8213707804679871,\n",
       "   0.8354834318161011,\n",
       "   0.491958349943161,\n",
       "   0.8099226355552673,\n",
       "   0.7046311497688293,\n",
       "   0.408498078584671,\n",
       "   0.48145586252212524,\n",
       "   0.6331765651702881,\n",
       "   0.488194078207016,\n",
       "   0.7828447222709656,\n",
       "   0.5321872234344482,\n",
       "   0.5415126085281372,\n",
       "   0.6624757051467896,\n",
       "   0.5990723967552185,\n",
       "   0.5455837845802307,\n",
       "   0.6722590327262878,\n",
       "   0.5806440711021423,\n",
       "   0.6416731476783752,\n",
       "   0.5269002318382263,\n",
       "   0.7419961094856262,\n",
       "   1.0000004768371582,\n",
       "   0.6512184143066406,\n",
       "   0.3580673336982727,\n",
       "   0.6807875633239746,\n",
       "   0.7312216758728027,\n",
       "   0.737505316734314,\n",
       "   1.0,\n",
       "   0.5777549743652344,\n",
       "   0.6377952098846436,\n",
       "   0.4960283935070038,\n",
       "   0.6151401996612549,\n",
       "   0.6979824900627136,\n",
       "   0.610174298286438,\n",
       "   0.7286696434020996,\n",
       "   0.9999998807907104,\n",
       "   0.8293885588645935,\n",
       "   1.000000238418579,\n",
       "   0.508824348449707,\n",
       "   0.3986209034919739,\n",
       "   0.5788242816925049,\n",
       "   0.6339024305343628,\n",
       "   0.4898231625556946,\n",
       "   0.4985845386981964,\n",
       "   0.6661220192909241,\n",
       "   0.5445133447647095,\n",
       "   0.5719353556632996,\n",
       "   1.0000001192092896,\n",
       "   0.5955889225006104,\n",
       "   0.5526111721992493,\n",
       "   0.5505340695381165,\n",
       "   0.5200064182281494,\n",
       "   0.626930832862854,\n",
       "   0.671001136302948,\n",
       "   0.5582233667373657,\n",
       "   0.7078972458839417,\n",
       "   0.6933188438415527,\n",
       "   0.9999995231628418,\n",
       "   0.8290351629257202,\n",
       "   0.5632892847061157,\n",
       "   0.43058282136917114,\n",
       "   0.5016887784004211,\n",
       "   0.527451753616333,\n",
       "   0.5771932601928711,\n",
       "   0.8038703799247742,\n",
       "   0.5685458183288574,\n",
       "   0.9999904632568359,\n",
       "   0.7229464054107666,\n",
       "   0.6983064413070679,\n",
       "   0.5501551032066345,\n",
       "   0.5189229846000671],\n",
       "  'f1': [0.9999998211860657,\n",
       "   0.5747026205062866,\n",
       "   0.6946502923965454,\n",
       "   0.7095690965652466,\n",
       "   0.6949440240859985,\n",
       "   0.6755099296569824,\n",
       "   0.6549558043479919,\n",
       "   0.6922680139541626,\n",
       "   0.7228438258171082,\n",
       "   0.5217258334159851,\n",
       "   0.9999997615814209,\n",
       "   0.6501652002334595,\n",
       "   1.0000001192092896,\n",
       "   0.7601302266120911,\n",
       "   0.7018850445747375,\n",
       "   0.6927808523178101,\n",
       "   0.604276180267334,\n",
       "   0.6221848130226135,\n",
       "   0.7307372093200684,\n",
       "   0.5827798843383789,\n",
       "   0.6561663746833801,\n",
       "   0.9416104555130005,\n",
       "   0.7169430255889893,\n",
       "   0.7644474506378174,\n",
       "   0.7055614590644836,\n",
       "   0.5370546579360962,\n",
       "   0.6808926463127136,\n",
       "   0.6132856607437134,\n",
       "   0.7701091766357422,\n",
       "   0.6849532723426819,\n",
       "   0.6994543671607971,\n",
       "   0.7516201138496399,\n",
       "   0.885439395904541,\n",
       "   0.5872480273246765,\n",
       "   0.6336464881896973,\n",
       "   0.7457402944564819,\n",
       "   0.6845422387123108,\n",
       "   0.5219218134880066,\n",
       "   0.5767869353294373,\n",
       "   1.000000238418579,\n",
       "   0.7464837431907654,\n",
       "   0.8254896402359009,\n",
       "   0.6924838423728943,\n",
       "   0.6845235228538513,\n",
       "   0.484041690826416,\n",
       "   0.6607378721237183,\n",
       "   0.7159644365310669,\n",
       "   0.6629581451416016,\n",
       "   0.9039372801780701,\n",
       "   0.5596942901611328,\n",
       "   0.6015090346336365,\n",
       "   0.6222954392433167,\n",
       "   0.5619606971740723,\n",
       "   0.6167296767234802,\n",
       "   0.7186709642410278,\n",
       "   0.6581975817680359,\n",
       "   0.6920795440673828,\n",
       "   0.7626417279243469,\n",
       "   0.5938730835914612,\n",
       "   0.9999998211860657,\n",
       "   0.6002998352050781,\n",
       "   0.7215941548347473,\n",
       "   0.6430563926696777,\n",
       "   0.718191385269165,\n",
       "   0.6185638308525085,\n",
       "   0.5616071224212646,\n",
       "   0.7251312732696533,\n",
       "   0.6349766254425049,\n",
       "   0.573667049407959,\n",
       "   0.6402536630630493,\n",
       "   0.6249964237213135,\n",
       "   0.5901507139205933,\n",
       "   0.9999998807907104,\n",
       "   0.7202579975128174,\n",
       "   0.5191221237182617,\n",
       "   0.7477887272834778,\n",
       "   0.6720578670501709,\n",
       "   0.6159363985061646,\n",
       "   0.9999998807907104,\n",
       "   0.6103855967521667,\n",
       "   0.6532455086708069,\n",
       "   0.6919999122619629,\n",
       "   0.5696225762367249,\n",
       "   0.6084505319595337,\n",
       "   0.625239908695221,\n",
       "   1.000000238418579,\n",
       "   0.6310200691223145,\n",
       "   0.644031286239624,\n",
       "   0.8046196699142456,\n",
       "   0.9999999403953552,\n",
       "   0.6313580870628357,\n",
       "   0.6447110176086426,\n",
       "   0.6943805813789368,\n",
       "   0.7575012445449829,\n",
       "   0.6473463773727417,\n",
       "   0.7181497812271118,\n",
       "   0.6183177828788757,\n",
       "   0.6341261863708496,\n",
       "   0.6435129046440125,\n",
       "   0.6680545806884766,\n",
       "   0.6377838850021362,\n",
       "   0.48806315660476685,\n",
       "   0.7909743785858154,\n",
       "   0.9999996423721313,\n",
       "   0.5628771185874939,\n",
       "   0.7732252478599548,\n",
       "   0.669996976852417,\n",
       "   0.6729815602302551,\n",
       "   0.8147409558296204,\n",
       "   0.7317805886268616,\n",
       "   0.555513858795166,\n",
       "   0.6450761556625366,\n",
       "   0.6436994075775146,\n",
       "   0.6560163497924805,\n",
       "   0.6787739396095276,\n",
       "   0.6736640930175781,\n",
       "   0.6409929394721985,\n",
       "   0.7845821380615234,\n",
       "   1.000000238418579,\n",
       "   0.5457825660705566,\n",
       "   0.554078221321106,\n",
       "   0.679560124874115,\n",
       "   1.0,\n",
       "   0.8572500348091125,\n",
       "   0.7390896677970886,\n",
       "   0.9999998211860657,\n",
       "   0.5283234119415283,\n",
       "   0.6531941294670105,\n",
       "   0.6620620489120483,\n",
       "   0.6292252540588379,\n",
       "   0.7011095285415649,\n",
       "   0.7044861316680908,\n",
       "   0.652056872844696,\n",
       "   0.5957585573196411,\n",
       "   0.473448246717453,\n",
       "   0.7262993454933167,\n",
       "   0.6661240458488464,\n",
       "   0.7426649928092957,\n",
       "   0.6820135712623596,\n",
       "   0.6604475975036621,\n",
       "   0.7615572214126587,\n",
       "   0.7506878972053528,\n",
       "   0.6633502840995789,\n",
       "   0.7131654620170593,\n",
       "   0.669390082359314,\n",
       "   0.5949956178665161,\n",
       "   0.7906418442726135,\n",
       "   0.647553026676178,\n",
       "   0.5986639261245728,\n",
       "   0.6859971284866333,\n",
       "   0.6926285624504089,\n",
       "   0.7527163028717041,\n",
       "   0.9999999403953552,\n",
       "   0.7213786244392395,\n",
       "   0.6621614098548889,\n",
       "   0.6919733285903931,\n",
       "   0.635905921459198,\n",
       "   0.8874790072441101,\n",
       "   0.4955607056617737,\n",
       "   0.6644071340560913,\n",
       "   0.6578807234764099,\n",
       "   0.6746957898139954,\n",
       "   0.6826174259185791,\n",
       "   0.9999996423721313,\n",
       "   0.7218232154846191,\n",
       "   0.6153052449226379,\n",
       "   0.6146332025527954,\n",
       "   0.6438013315200806,\n",
       "   0.5858286023139954,\n",
       "   0.5982457995414734,\n",
       "   1.000000238418579,\n",
       "   0.672705352306366,\n",
       "   0.6472417116165161,\n",
       "   0.7336524724960327,\n",
       "   0.6812138557434082,\n",
       "   0.6822936534881592,\n",
       "   0.6387922167778015,\n",
       "   0.511875569820404,\n",
       "   0.5821818113327026,\n",
       "   1.0,\n",
       "   0.6245966553688049,\n",
       "   0.7661259174346924,\n",
       "   0.50175541639328,\n",
       "   0.6265211701393127,\n",
       "   0.8947324752807617,\n",
       "   0.6861444711685181,\n",
       "   0.9371755123138428,\n",
       "   0.6472473740577698,\n",
       "   0.5924638509750366,\n",
       "   0.5732221603393555,\n",
       "   0.7044886350631714,\n",
       "   1.000000238418579,\n",
       "   0.6407907605171204,\n",
       "   0.6848832368850708,\n",
       "   0.7018922567367554,\n",
       "   0.685137927532196,\n",
       "   0.6966654658317566,\n",
       "   0.723560094833374,\n",
       "   0.901068925857544,\n",
       "   0.7902723550796509,\n",
       "   0.625557005405426,\n",
       "   0.7637507915496826,\n",
       "   0.7544674873352051,\n",
       "   0.5246496796607971,\n",
       "   0.6125751733779907,\n",
       "   0.7285532355308533,\n",
       "   0.6192096471786499,\n",
       "   0.7131560444831848,\n",
       "   0.5609848499298096,\n",
       "   0.6404508948326111,\n",
       "   0.6325852870941162,\n",
       "   0.6954444646835327,\n",
       "   0.549316942691803,\n",
       "   0.671007513999939,\n",
       "   0.693832516670227,\n",
       "   0.7317430377006531,\n",
       "   0.6896547675132751,\n",
       "   0.7419961094856262,\n",
       "   1.0000004768371582,\n",
       "   0.7881187796592712,\n",
       "   0.5273189544677734,\n",
       "   0.7312008738517761,\n",
       "   0.772032618522644,\n",
       "   0.7622716426849365,\n",
       "   1.0,\n",
       "   0.731810986995697,\n",
       "   0.7113426923751831,\n",
       "   0.6626632213592529,\n",
       "   0.48613759875297546,\n",
       "   0.6979824900627136,\n",
       "   0.709203839302063,\n",
       "   0.7744889855384827,\n",
       "   0.9999998807907104,\n",
       "   0.9058720469474792,\n",
       "   1.000000238418579,\n",
       "   0.6744646430015564,\n",
       "   0.5345089435577393,\n",
       "   0.6823127269744873,\n",
       "   0.7095453143119812,\n",
       "   0.6571027636528015,\n",
       "   0.615693986415863,\n",
       "   0.7112865447998047,\n",
       "   0.560314953327179,\n",
       "   0.5982486009597778,\n",
       "   1.0000001192092896,\n",
       "   0.6965906023979187,\n",
       "   0.5655714869499207,\n",
       "   0.6426498293876648,\n",
       "   0.6331648230552673,\n",
       "   0.6187760829925537,\n",
       "   0.5640760064125061,\n",
       "   0.7159457206726074,\n",
       "   0.7528424263000488,\n",
       "   0.7641214728355408,\n",
       "   0.9999995231628418,\n",
       "   0.905661404132843,\n",
       "   0.6708051562309265,\n",
       "   0.6019682884216309,\n",
       "   0.6112247109413147,\n",
       "   0.650378406047821,\n",
       "   0.5753012895584106,\n",
       "   0.6860733032226562,\n",
       "   0.6563870906829834,\n",
       "   0.9064182639122009,\n",
       "   0.7215250134468079,\n",
       "   0.5659155249595642,\n",
       "   0.7092755436897278,\n",
       "   0.6827856302261353],\n",
       "  'hashcode': 'microsoft/deberta-base_L9_no-idf_version=0.3.12(hug_trans=4.29.2)'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = utils.bert.compute_metrics(\n",
    "    predictions[\"start_logits\"],\n",
    "    predictions[\"end_logits\"],\n",
    "    test_dataset,\n",
    "    raw_test,\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# question_answerer = pipeline(\"question-answering\", model=SAVE_CHECKPOINT_PATH, tokenizer=TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = question_answerer(question=raw_test[\"question\"], context=raw_test[\"context\"])\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_answers = [prediction[\"answer\"] for prediction in predictions]\n",
    "# expected_answers = [answer[\"text\"][0] for answer in raw_test[\"answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# meteor = evaluate.load(\"meteor\")\n",
    "# bleu = evaluate.load(\"bleu\")\n",
    "# bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meteor_results = meteor.compute(predictions=predicted_answers, references=expected_answers)\n",
    "# bleu_results = bleu.compute(predictions=predicted_answers, references=expected_answers)\n",
    "# bertscore_results = bertscore.compute(predictions=predicted_answers, references=expected_answers, lang=\"en\", model_type=\"microsoft/deberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Meteor: {meteor_results['meteor']}\\nBLEU-4: {bleu_results['bleu']}\\nBERTscore Mean F1: {sum(bertscore_results['f1'])/len(bertscore_results['f1'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, answer in enumerate(expected_answers):\n",
    "#     print(f\"Expected: {answer} - Predicted: {predicted_answers[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
